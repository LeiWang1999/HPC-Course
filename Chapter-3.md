1. 从加速比的基本定义出发，证明Amdahl定律和Gustafson定律。

加速比的基本定义：
$$
S_p(q) = \frac{T_s}{T_p(q)}
$$
其中$T_s$表示并行之前的执行速度，$T_p(q)$表示并行之后的执行速度，其中q表示并行度。

Amdahl定律:

假设程序中必须串行执行a%的程序，则可以并行的程序为(1-a)%，则总的并行运行时间可以表示为$aT_s+(1-a)T_s/p$，则
$$
S_p(q) = \frac{1}{a+(1-a)/p}
$$
当并行设备数量趋近于正无穷，则最大的加速比为$\frac{1}{a}$.

Guastafson定律:

假设并行程序中必须串行执行a%的程序，则可以并行的程序为(1-a)%，则串行程序总的运行时间可以表示为$aT_p+(1-a)*p*T_p$，​则
$$
S_p(q) = \frac{a+(1-a)*p}{1}
$$


2. 你所了解的并行计算基本方法。

1. 区域分解方法

   包括非重叠的区域分解和带重叠的区域分解方法等，主要用来求解椭圆微分方程的近似解等问题。

2. 功能分解方法

   将不同功能组成的问题，按照其功能进行分解后并行计算的方法。

3. 流水线技术

   流水线技术是并行计算中非常有效常用的手段，多用来解决递推式计算等场景，典型的例子如5点差分格式的Gauss-Seidel迭代，二维或三维的PDE数值解等）

4. 分而治之法

   将大问题分解成几个相互无依赖的小问题分开进行并行计算的方法，分而治之法在并行计算中起着举足轻重的作用。

5. 同步并行算法

   为了计算下一步，所有处理器必须都执行完成这一步之后才能继续进行吗这就是典型的同步并行算法，从串行算法直接并行化得到的算法多数都是同步并行算法，前文所述的流水线算法，分而治之算法等在某种意义上也是同步并行算法。

6. 异步并行算法

   异步并行算法是相对同步并行算法提出的，异步并行算法数据交换等并不严格限定在某一时刻，每个处理器按照预定的计算任务持续执行，没有同步并行式的频繁全局同步，不需要活很少处理器等待，异步并行的典型案例是并行遗传算法中的分解放啊，进程间由随机数控制是否发生通信等。

Mpi，主要用于多台设备之间的分布式并行计算。

Openmp，主要用于线程级别的并行计算。

Cuda，Nvidia GPU提供的并行计算库。
